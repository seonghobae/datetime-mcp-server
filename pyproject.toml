[project]
name = "datetime-mcp-server"
version = "0.1.0"
description = "A MCP server project"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
 "anthropic>=0.45.2",
 "fastapi>=0.116.0",
 "hypercorn[h2,h3,trio,uvloop]>=0.17.3",
 "mcp[cli]>=1.2.1",
 "python-dotenv>=1.0.1",
 "uvloop>=0.21.0",
]
[[project.authors]]
name = "Malcolm Jones"
email = "bossjones@theblacktonystark.com"

[build-system]
requires = [ "hatchling",]
build-backend = "hatchling.build"

[dependency-groups]
dev = [
    "coverage[toml]>=7.6.10",
    "debugpy>=1.8.12",
    "mock>=5.1.0",
    "pre-commit>=4.1.0",
    "psutil>=7.0.0",
    "pydocstyle>=6.3.0",
    "pyright>=1.1.393",
    "pytest>=8.3.4",
    "pytest-aiohttp>=1.1.0",
    "pytest-aioresponses>=0.3.0",
    "pytest-asyncio>=0.25.3",
    "pytest-benchmark>=5.1.0",
    "pytest-clarity>=1.0.1",
    "pytest-cov>=6.0.0",
    "pytest-httpx>=0.35.0",
    "pytest-ignore-flaky>=2.2.1",
    "pytest-memray>=1.7.0",
    "pytest-mock>=3.14.0",
    "pytest-recording>=0.13.2",
    "pytest-retry>=1.7.0",
    "pytest-skip-slow>=0.0.5",
    "pytest-skipuntil>=0.2.0",
    "pytest-sugar>=1.0.0",
    "pytest-timeout>=2.4.0",
    "pyupgrade>=3.19.1",
    "requests-mock>=1.12.1",
    "respx>=0.22.0",
    "ruff>=0.9.5",
    "tox-uv>=1.22.1",
    "types-aiofiles>=24.1.0.20241221",
    "types-beautifulsoup4>=4.12.0.20250204",
    "types-mock>=5.1.0.20240425",
    "types-requests>=2.32.0.20241016",
    "validate-pyproject[all,store]>=0.23",
]

[project.scripts]
datetime-mcp-server = "datetime_mcp_server:main"

[tool.pytest.ini_options]
markers = [
    "benchmark: marks tests as benchmark tests",
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "sse: marks tests as server-sent events tests",
    "http: marks tests as HTTP transport tests"
]
# Performance benchmarking configuration
addopts = [
    "--benchmark-columns=min,max,mean,stddev,median,rounds,iterations",
    "--benchmark-sort=mean",
    "--benchmark-warmup=off",
    "--benchmark-timer=time.perf_counter",
    "--benchmark-min-time=0.000005",
    "--benchmark-max-time=0.050",
    "--benchmark-min-rounds=5",
    "--benchmark-json=benchmark_results.json",
    # Timeout settings optimized for different test types
    "--timeout=60",  # Default 1 minute for regular tests
    "--timeout-method=thread",  # Use thread-based timeout for better compatibility
    "-v"  # Verbose output for better debugging
]
# Global timeout settings with specific overrides
timeout = 60  # Default 1 minute per test (reduced from 5 minutes)
timeout_method = "thread"  # Use thread timeout method

# Test collection and execution optimization
asyncio_mode = "auto"  # Automatic asyncio mode detection
asyncio_default_fixture_loop_scope = "function"  # Function-scoped event loops
